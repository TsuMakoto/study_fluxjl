# MNITSã‚’ã‚‚ã£ã¦ã€Flux.jlã‚’å…¥é–€ã™ã‚‹(2)

[Julia Advent Calendar 2019](https://qiita.com/advent-calendar/2019/julialang)ã®æœ€çµ‚ç›®ã®è¨˜äº‹ã§ã™ã€‚

ã¨ã„ã£ã¦ã‚‚å‰å›ã®ç¶šãã¿ãŸã„ãªå½¢ã¨ãªã£ã¦ã—ã¾ã„ã€‚æç¸®ã§ã™ã€‚

æœ¬å½“ã¯Turing.jlã«ã¤ã„ã¦qiitaã§æ›¸ã“ã†ã‹ã¨æ€ã£ã¦ã¾ã—ãŸãŒã€ã¨ã‚Šã‚ãˆãšä¸€å€‹ç¢ºå®Ÿã«ä½¿ãˆã‚‹ã‚ˆã†ã«ãªã‚ŠãŸã‹ã£ãŸã®ã§ã“ã‚Œã«ã—ã¾ã—ãŸã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒé–“é•ã£ã¦ãŸ

[å‰å›](../1/index.html)ã®è¨˜äº‹ã§ã¯ã€ãªã‚“ã‹ã‚ˆãã‚ã‹ã‚‰ãšã€æ¨è«–ãŒã†ã¾ãè¡Œã‹ãªã‹ã£ãŸã®ã§ã™ãŒã€ã‚ˆãã‚ˆãè¦‹ã‚‹ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ‰±ã„ã‚’å‹˜é•ã„ã—ã¦ã¾ã—ãŸã€‚

## åŸºæœ¬çš„ãªäº‹ã¯åŒã˜ã ã£ãŸã‚“ã ã‚ˆãªã‚

ä»¥ä¸‹ã®éƒ¨åˆ†ã¾ã§ã¯ã»ã¨ã‚“ã©å•é¡Œãªã‹ã£ãŸã§ã™ã€‚


```julia
# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®èª­è¾¼
using Flux
using Flux.Data.MNIST
imgs = MNIST.images(:train);

using Flux: onehotbatch
imgs = MNIST.images()
train_X = hcat(float.(vec.(imgs))...)
labels = MNIST.labels(:train)
train_Y = onehotbatch(labels, 0:9)

# å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
using Flux: Chain
using Flux: Dense
using NNlib: softmax
using NNlib: relu
layeri_1 = Dense(28^2, 32, relu)
layer2_o = Dense(32, 10)
model = Chain(layeri_1, layer2_o, softmax)

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’32å€‹ãšã¤ã«åˆ†å‰²
using Base.Iterators: partition
batchsize = 32
serial_iterator = partition(1:size(train_Y)[2], batchsize)
train_dataset = map(batch -> (train_X[:, batch], train_Y[:, batch]), serial_iterator);
```


```julia
size(train_dataset[1][1]), size(train_dataset[1][2])
```




    ((784, 32), (10, 32))



ä»Šã€ä¸Šã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚
ã“ã“ã§ã€[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://fluxml.ai/Flux.jl/stable/training/training/#Datasets-1)ã®ä¾‹ã‚’ã¿ã¦ã¿ã‚‹ã¨

---
```julia
x = rand(784)
y = rand(10)
data = [(x, y), (x, y), (x, y)]
# Or equivalently
data = Iterators.repeated((x, y), 3)
```
---
ã¨ã€ä¸Šã®ã‚ˆã†ã«ãªã£ã¦ã¾ã™ã€‚

è‡ªåˆ†ã®ç†è§£ã§ã¯ã€`((Xã®ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒ, batchã‚µã‚¤ã‚º), (Yã®ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒ, batchã‚µã‚¤ã‚º))`ã§å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ã‚‚ã®ã‹ã¨æ€ã£ã¦ã¾ã—ãŸã€‚

ç´”ç²‹ã« `(Xã®ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒ, Yã®ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒ)`ã‚’å¼•æ•°ã«ä¸ãˆã‚‹ã‚“ã§ã™ã­ã€‚

ã‚„ã¯ã‚Šã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã¿ã‚‹ã®ã¯å¤§äº‹ã€‚

## ãƒ‡ãƒ¼ã‚¿ã‚’ä½œã‚Šç›´ã—

ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œã‚Šæ–¹ã¯[genkuroki/using Flux.jl.ipynb](https://gist.github.com/genkuroki/49bdba858d4b6c7020f463c648e309f3)ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™ã€‚

ã„ã¤ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ğŸ™‡â€â™‚ï¸


```julia
dataset = Iterators.repeated((train_X, train_Y), 200)
evalcb = () -> @show(loss(train_X, train_Y))
```




    #43 (generic function with 1 method)



---
æ™®é€šã¯ä¸€ã¤ãšã¤X,Yã‚’è¨­å®šã™ã‚‹ãŸã‚ã€ä¸Šã®ã‚ˆã†ã«ãã‚Œãã‚Œã§å®šç¾©ã—ã¾ã™ã€‚

ãã—ã¦ã€ãã‚Œã‚’datasetã‚’æº–å‚™ã™ã‚‹ã¨ã€`(å…¥åŠ›ç”»åƒã®æ¬¡å…ƒæ•°, å‡ºåŠ›çµæœã®æ¬¡å…ƒæ•°) = (784, 10)`ã®å½¢ã§å–å¾—ã§ãã¾ã™ã€‚

ã“ã‚Œã§ã§ãã‚‹ã¯ãš!!

## å­¦ç¿’ã‚’ã™ã‚‹


```julia
using Flux: crossentropy
using Flux: ADAM
using Flux: train!

opt = ADAM()
loss(x, y) = crossentropy(model(x), y)
train!(loss, Flux.params(model), dataset, opt, cb = Flux.throttle(evalcb, 10))
```

    loss(train_X, train_Y) = 2.3338146f0
    loss(train_X, train_Y) = 2.0817525f0
    loss(train_X, train_Y) = 1.8778219f0
    loss(train_X, train_Y) = 1.6883554f0
    loss(train_X, train_Y) = 1.5142949f0
    loss(train_X, train_Y) = 1.3592445f0
    loss(train_X, train_Y) = 1.2230834f0
    loss(train_X, train_Y) = 1.1031209f0
    loss(train_X, train_Y) = 0.9976298f0
    loss(train_X, train_Y) = 0.90592045f0
    loss(train_X, train_Y) = 0.8274122f0
    loss(train_X, train_Y) = 0.76068306f0
    loss(train_X, train_Y) = 0.70442736f0
    loss(train_X, train_Y) = 0.65698516f0
    loss(train_X, train_Y) = 0.6165953f0
    loss(train_X, train_Y) = 0.5819991f0
    loss(train_X, train_Y) = 0.55228555f0
    loss(train_X, train_Y) = 0.5266146f0
    loss(train_X, train_Y) = 0.5042526f0
    loss(train_X, train_Y) = 0.48459056f0
    loss(train_X, train_Y) = 0.4671772f0
    loss(train_X, train_Y) = 0.4516567f0
    loss(train_X, train_Y) = 0.4377649f0
    loss(train_X, train_Y) = 0.42524526f0
    loss(train_X, train_Y) = 0.41390404f0
    loss(train_X, train_Y) = 0.40357998f0
    loss(train_X, train_Y) = 0.39413762f0
    loss(train_X, train_Y) = 0.38546035f0
    loss(train_X, train_Y) = 0.37745515f0
    loss(train_X, train_Y) = 0.37004215f0
    loss(train_X, train_Y) = 0.36315277f0
    loss(train_X, train_Y) = 0.35672128f0
    loss(train_X, train_Y) = 0.35069942f0
    loss(train_X, train_Y) = 0.34504107f0
    loss(train_X, train_Y) = 0.3397092f0
    loss(train_X, train_Y) = 0.33466172f0
    loss(train_X, train_Y) = 0.32986608f0
    loss(train_X, train_Y) = 0.32527816f0
    loss(train_X, train_Y) = 0.32087335f0
    loss(train_X, train_Y) = 0.31662682f0
    loss(train_X, train_Y) = 0.31253207f0
    loss(train_X, train_Y) = 0.30859295f0
    loss(train_X, train_Y) = 0.30481726f0
    loss(train_X, train_Y) = 0.30115998f0
    loss(train_X, train_Y) = 0.29763132f0
    loss(train_X, train_Y) = 0.29422432f0
    loss(train_X, train_Y) = 0.2909183f0
    loss(train_X, train_Y) = 0.28770086f0
    loss(train_X, train_Y) = 0.28456584f0
    loss(train_X, train_Y) = 0.28150916f0


## æ¨è«–ã—ã¦ã¿ã‚‹
ã†ã¾ãè¡Œã£ã¦ãã‚Œã‚ˆ


```julia
using Statistics: mean
test_X = hcat(float.(vec.(MNIST.images(:test)))...)
test_Y = onehotbatch(MNIST.labels(:test), 0:9)
mean(onecold(model(test_X)) .== onecold(test_Y))
```




    0.9239



ãŠã€ã„ã„ã£ã™ã­ï¼ï¼

ã‚ˆã†ã‚„ãã€ã“ã‚Œã§å…¥é–€ã§ããŸã‹ãªã€‚ã€‚ã€‚

## ãŠã‚ã‚Šã«
æ¬¡ã¯irisã¨ã‹ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³MNISTã¨ã‹ã‚„ã£ã¦ã¿ã‚ˆã†ã‹ãª

ã¨ã‚Šã‚ãˆãšã€ã‚¢ãƒ‰ã‚«ãƒ¬å®Œèµ°ã—ãŸï¼ï¼

èµ°ã‚Šåˆ‡ã£ãŸï¼
