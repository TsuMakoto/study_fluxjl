{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNITSをもって、Flux.jlを入門する(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Julia Advent Calendar 2019](https://qiita.com/advent-calendar/2019/julialang)の最終目の記事です。\n",
    "\n",
    "といっても前回の続きみたいな形となってしまい。恐縮です。\n",
    "\n",
    "本当はTuring.jlについてqiitaで書こうかと思ってましたが、とりあえず一個確実に使えるようになりたかったのでこれにしました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットが間違ってた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[前回](../1/index.html)の記事では、なんかよくわからず、推論がうまく行かなかったのですが、よくよく見るとデータセットの扱いを勘違いしてました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的な事は同じだったんだよなあ\n",
    "\n",
    "以下の部分まではほとんど問題なかったです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの読込\n",
    "using Flux\n",
    "using Flux.Data.MNIST\n",
    "imgs = MNIST.images(:train);\n",
    "\n",
    "using Flux: onehotbatch\n",
    "imgs = MNIST.images()\n",
    "train_X = hcat(float.(vec.(imgs))...)\n",
    "labels = MNIST.labels(:train)\n",
    "train_Y = onehotbatch(labels, 0:9)\n",
    "\n",
    "# 学習モデルの定義\n",
    "using Flux: Chain\n",
    "using Flux: Dense\n",
    "using NNlib: softmax\n",
    "using NNlib: relu\n",
    "layeri_1 = Dense(28^2, 32, relu)\n",
    "layer2_o = Dense(32, 10)\n",
    "model = Chain(layeri_1, layer2_o, softmax)\n",
    "\n",
    "# 訓練データを32個ずつに分割\n",
    "using Base.Iterators: partition\n",
    "batchsize = 32\n",
    "serial_iterator = partition(1:size(train_Y)[2], batchsize)\n",
    "train_dataset = map(batch -> (train_X[:, batch], train_Y[:, batch]), serial_iterator);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 32), (10, 32))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_dataset[1][1]), size(train_dataset[1][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今、上のようになっています。\n",
    "ここで、[ドキュメント](https://fluxml.ai/Flux.jl/stable/training/training/#Datasets-1)の例をみてみると\n",
    "\n",
    "---\n",
    "```julia\n",
    "x = rand(784)\n",
    "y = rand(10)\n",
    "data = [(x, y), (x, y), (x, y)]\n",
    "# Or equivalently\n",
    "data = Iterators.repeated((x, y), 3)\n",
    "```\n",
    "---\n",
    "と、上のようになってます。\n",
    "\n",
    "自分の理解では、`((Xのデータの次元, batchサイズ), (Yのデータの次元, batchサイズ))`で学習を実行するものかと思ってました。\n",
    "\n",
    "純粋に `(Xのデータの次元, Yのデータの次元)`を引数に与えるんですね。\n",
    "\n",
    "やはり、ドキュメントをみるのは大事。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データを作り直し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下のデータセットの作り方は[genkuroki/using Flux.jl.ipynb](https://gist.github.com/genkuroki/49bdba858d4b6c7020f463c648e309f3)を参考にしています。\n",
    "\n",
    "いつもありがとうございます🙇‍♂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#43 (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Iterators.repeated((train_X, train_Y), 200)\n",
    "evalcb = () -> @show(loss(train_X, train_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "普通は一つずつX,Yを設定するため、上のようにそれぞれで定義します。\n",
    "\n",
    "そして、それをdatasetを準備すると、`(入力画像の次元数, 出力結果の次元数) = (784, 10)`の形で取得できます。\n",
    "\n",
    "これでできるはず!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss(train_X, train_Y) = 2.3338146f0\n",
      "loss(train_X, train_Y) = 2.0817525f0\n",
      "loss(train_X, train_Y) = 1.8778219f0\n",
      "loss(train_X, train_Y) = 1.6883554f0\n",
      "loss(train_X, train_Y) = 1.5142949f0\n",
      "loss(train_X, train_Y) = 1.3592445f0\n",
      "loss(train_X, train_Y) = 1.2230834f0\n",
      "loss(train_X, train_Y) = 1.1031209f0\n",
      "loss(train_X, train_Y) = 0.9976298f0\n",
      "loss(train_X, train_Y) = 0.90592045f0\n",
      "loss(train_X, train_Y) = 0.8274122f0\n",
      "loss(train_X, train_Y) = 0.76068306f0\n",
      "loss(train_X, train_Y) = 0.70442736f0\n",
      "loss(train_X, train_Y) = 0.65698516f0\n",
      "loss(train_X, train_Y) = 0.6165953f0\n",
      "loss(train_X, train_Y) = 0.5819991f0\n",
      "loss(train_X, train_Y) = 0.55228555f0\n",
      "loss(train_X, train_Y) = 0.5266146f0\n",
      "loss(train_X, train_Y) = 0.5042526f0\n",
      "loss(train_X, train_Y) = 0.48459056f0\n",
      "loss(train_X, train_Y) = 0.4671772f0\n",
      "loss(train_X, train_Y) = 0.4516567f0\n",
      "loss(train_X, train_Y) = 0.4377649f0\n",
      "loss(train_X, train_Y) = 0.42524526f0\n",
      "loss(train_X, train_Y) = 0.41390404f0\n",
      "loss(train_X, train_Y) = 0.40357998f0\n",
      "loss(train_X, train_Y) = 0.39413762f0\n",
      "loss(train_X, train_Y) = 0.38546035f0\n",
      "loss(train_X, train_Y) = 0.37745515f0\n",
      "loss(train_X, train_Y) = 0.37004215f0\n",
      "loss(train_X, train_Y) = 0.36315277f0\n",
      "loss(train_X, train_Y) = 0.35672128f0\n",
      "loss(train_X, train_Y) = 0.35069942f0\n",
      "loss(train_X, train_Y) = 0.34504107f0\n",
      "loss(train_X, train_Y) = 0.3397092f0\n",
      "loss(train_X, train_Y) = 0.33466172f0\n",
      "loss(train_X, train_Y) = 0.32986608f0\n",
      "loss(train_X, train_Y) = 0.32527816f0\n",
      "loss(train_X, train_Y) = 0.32087335f0\n",
      "loss(train_X, train_Y) = 0.31662682f0\n",
      "loss(train_X, train_Y) = 0.31253207f0\n",
      "loss(train_X, train_Y) = 0.30859295f0\n",
      "loss(train_X, train_Y) = 0.30481726f0\n",
      "loss(train_X, train_Y) = 0.30115998f0\n",
      "loss(train_X, train_Y) = 0.29763132f0\n",
      "loss(train_X, train_Y) = 0.29422432f0\n",
      "loss(train_X, train_Y) = 0.2909183f0\n",
      "loss(train_X, train_Y) = 0.28770086f0\n",
      "loss(train_X, train_Y) = 0.28456584f0\n",
      "loss(train_X, train_Y) = 0.28150916f0\n"
     ]
    }
   ],
   "source": [
    "using Flux: crossentropy\n",
    "using Flux: ADAM\n",
    "using Flux: train!\n",
    "\n",
    "opt = ADAM()\n",
    "loss(x, y) = crossentropy(model(x), y)\n",
    "train!(loss, Flux.params(model), dataset, opt, cb = Flux.throttle(evalcb, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論してみる\n",
    "うまく行ってくれよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9239"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics: mean\n",
    "test_X = hcat(float.(vec.(MNIST.images(:test)))...)\n",
    "test_Y = onehotbatch(MNIST.labels(:test), 0:9)\n",
    "mean(onecold(model(test_X)) .== onecold(test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "お、いいっすね！！\n",
    "\n",
    "ようやく、これで入門できたかな。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## おわりに\n",
    "次はirisとかファッションMNISTとかやってみようかな\n",
    "\n",
    "とりあえず、アドカレ完走した！！\n",
    "\n",
    "走り切った！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
